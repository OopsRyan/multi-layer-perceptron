{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwwiGNvy69lB"
   },
   "source": [
    "# **COMP30230**\n",
    "\n",
    "# **Qirun** **Chen** - **Student** **No. 16212138**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zloWLBlInA4B"
   },
   "source": [
    "# Multi layer perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "N-UMZYBMfP32"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Layer Perceptron - only built with a single hidden layer\n",
    "\n",
    "An assignment of Connectionist Computing - UCD COMP30230 Module\n",
    "\n",
    "- Support learning the XOR problem\n",
    "- Support learning the Sin function\n",
    "- Support identifying handwritten letters trained by the data set below\n",
    "    Link => http://archive.ics.uci.edu/ml/datasets/Letter+Recognition\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.1'\n",
    "__author__ = 'Qirun Chen - Student No. 16212138'\n",
    "__date__ = '27 Apr 2018'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_i, n_h, n_o, activation='sigmoid', max_epochs=5000,\n",
    "                 learning_rate=0.1, verbose=2):\n",
    "        \"\"\" Initialize the whole network\n",
    "        :param n_i: The number of units in the input layer\n",
    "        :param n_h: The number of units in the hidden layer\n",
    "        :param n_o: The number of units in the output layer\n",
    "        :param activation: The activation function for activating the weights and inputs\n",
    "        :param max_epochs: The maximum iterations to training the MLP, to adjust weights\n",
    "        :param learning_rate: Indicating how much the delta on weights would be accepted\n",
    "        :param verbose: How much details should be printed during the training\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_i = n_i\n",
    "        self.n_h = n_h\n",
    "        self.n_o = n_o\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = np.power(10, verbose-1)\n",
    "        # Initialize the activation function and its derivative functions\n",
    "        self.activation, self.d_activation = \\\n",
    "            self.__initialize_activation_func(activation)\n",
    "\n",
    "        # Initialize the loss function\n",
    "        self.__initialize_loss_func()\n",
    "\n",
    "        # Initialize the units to hold activation results\n",
    "        self.input = np.ones(self.n_i + 1)  # 1 for bias\n",
    "        self.h = np.ones(self.n_h)\n",
    "        self.o = np.ones(self.n_o)\n",
    "\n",
    "        # Initialize the weights of the lower layer and the upper layer\n",
    "        self.W1, self.W2 = self.__initialize_weights()\n",
    "\n",
    "    def __initialize_activation_func(self, activation):\n",
    "        \"\"\" Initialize the activation function and its derivative function\n",
    "        :param activation: The symbol of activation, taking a string\n",
    "        :return: The assigned activation function and its derivative\n",
    "        \"\"\"\n",
    "        # Temporarily using tanh - a rescaled logistic sigmoid function\n",
    "        return tanh, d_tanh\n",
    "\n",
    "    def __initialize_loss_func(self):\n",
    "        \"\"\" Initialize the loss function\n",
    "        - when n_o > 1, classification should apply cross entropy\n",
    "        - when n_o = 1, like a regression problem, should use squared error\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.n_o > 1:\n",
    "            self.loss_func = cross_entropy\n",
    "        else:\n",
    "            self.loss_func = squared_error\n",
    "\n",
    "    def __initialize_weights(self):\n",
    "        \"\"\" Randomly initialize weights between -1 and 1\n",
    "                for the lower layer and the upper layer\n",
    "        :return: The initialized weights\n",
    "        \"\"\"\n",
    "        w1 = np.random.uniform(-0.2, 0.2, (self.input.size, self.h.size))\n",
    "        w2 = np.random.uniform(-0.2, 0.2, (self.h.size, self.o.size))\n",
    "        return w1, w2\n",
    "\n",
    "    def __forwards(self, inputs):\n",
    "        \"\"\" Propagate the inputs forward from the input layer\n",
    "                to the output layer\n",
    "        :param inputs: The training example\n",
    "        :return: The output on output units\n",
    "        \"\"\"\n",
    "\n",
    "        # In the input layer, the last one is 1 for bias\n",
    "        self.input[:-1] = inputs\n",
    "        # Activate the hidden layer\n",
    "        self.h = self.activation(np.dot(self.input, self.W1))\n",
    "        if self.n_o > 1:\n",
    "            # Classification - using SoftMax\n",
    "            self.o = softmax(np.dot(self.h, self.W2))\n",
    "        else:\n",
    "            # Regression\n",
    "            self.o = self.activation(np.dot(self.h, self.W2))\n",
    "\n",
    "        return self.o\n",
    "\n",
    "    def __backwards(self, expected):\n",
    "        \"\"\" The core training technique.\n",
    "                Propagate the error signal back to each layer,\n",
    "                and adjust the weights\n",
    "        :param expected: The expected outputs for calculating the error\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # The error on the output layer\n",
    "        error = expected - self.o\n",
    "        # Computing the delta activation of the output layer\n",
    "        if self.n_o > 1:\n",
    "            # Classification\n",
    "            dz2 = error * d_softmax(self.o, softmax)\n",
    "        else:\n",
    "            # Regression\n",
    "            dz2 = error * self.d_activation(self.o)\n",
    "        # Computing the delta activation of the hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.d_activation(self.h)\n",
    "\n",
    "        # Update the weights\n",
    "        self.__update_weights(dz1, dz2)\n",
    "\n",
    "    def __update_weights(self, dz1, dz2):\n",
    "        \"\"\" Update weights on the lower layer and the upper layer\n",
    "        :param dz1: The delta on the hidden layer\n",
    "        :param dz2: The delta on the output layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        dw1 = np.dot(np.atleast_2d(self.input).T, np.atleast_2d(dz1))\n",
    "        self.W1 += self.learning_rate * dw1\n",
    "        dw2 = np.dot(np.atleast_2d(self.h).T, np.atleast_2d(dz2))\n",
    "        self.W2 += self.learning_rate * dw2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Training the MLP network by the training set\n",
    "        Adjust the weights by iterate the training set max-epoch times\n",
    "        :param X: The features of the training set\n",
    "        :param y: The labels/Outputs of the training set\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        for e in range(1, self.max_epochs):\n",
    "            cost = 0.\n",
    "            for j, row in enumerate(X):\n",
    "                # feed-forward the inputs to the output layer\n",
    "                o = self.__forwards(row)\n",
    "                # Accumulate the error of each example computed\n",
    "                #   by the loss function to get the cost\n",
    "                cost += self.loss_func(o, y[j])\n",
    "                # Back-propagate the error signal computed\n",
    "                #   according to the given expected output\n",
    "                self.__backwards(y[j])\n",
    "\n",
    "            # Print details during training\n",
    "            if self.n_o > 1:\n",
    "                # Classification\n",
    "                # Predict the training set and print the current accuracy\n",
    "                pre = self.predict(X)\n",
    "                acc = 0.\n",
    "                for k, _ in enumerate(y):\n",
    "                    if pre[k] == np.argmax(y[k]):\n",
    "                        acc += 1\n",
    "                if e % self.verbose == 0:\n",
    "                    print('epoch %d | error : %.3f | accuracy : %.3f' %\n",
    "                          (e, cost/len(X), acc/len(X)))\n",
    "            else:\n",
    "                if e % self.verbose == 0:\n",
    "                    # Regression - print the error\n",
    "                    print('epoch %d | error : %.3f' % (e, cost/(len(X))))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict on the test set\n",
    "        :param X: The unseen features of the test set\n",
    "        :return: The predicted output for each example in the test set\n",
    "        \"\"\"\n",
    "        y = list()\n",
    "        for j, row in enumerate(X):\n",
    "            if self.n_o > 1:\n",
    "                # Classification - using one hot encoding,\n",
    "                #   so find the index of output units with the max output\n",
    "                y.append(np.argmax(self.__forwards(row)))\n",
    "            else:\n",
    "                y.append(self.__forwards(row))\n",
    "        return np.array(y)\n",
    "\n",
    "\n",
    "# Definitions of sigmoid, derivatives, SoftMax, and loss functions\n",
    "def tanh(x):\n",
    "    \"\"\" The rescaled logistic sigmoid function\n",
    "    :param x: The input needs to be activated\n",
    "    :return: corresponding tanh value with the input x\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def d_tanh(x):\n",
    "    \"\"\" The derivative of the tanh activation function\n",
    "    :param x: The input value\n",
    "    :return: The derivative\n",
    "    \"\"\"\n",
    "    return 1.0 - x**2\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Compute the SoftMax of vector x in a numerically stable way,\n",
    "            since numpy's exp would lead to infinite (nan)\n",
    "    :param x: In Classification, the inputs on the output units need to be activated\n",
    "    :return: The probabilities on each output unit. The sum should be 1.\n",
    "    \"\"\"\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def d_softmax(o, f):\n",
    "    \"\"\" The derivative of SoftMax with respect to the output\n",
    "    :param o: The output\n",
    "    :param f: The activation function of the output layer - SoftMax in classification\n",
    "    :return: The derivative\n",
    "    \"\"\"\n",
    "    return f(o) * (1 - f(o))\n",
    "\n",
    "\n",
    "def cross_entropy(o, y):\n",
    "    \"\"\" Cross entropy loss function\n",
    "    :param o: The output\n",
    "    :param y: The expected output\n",
    "    :return: The cross entropy - the loss\n",
    "    \"\"\"\n",
    "    return np.sum(np.nan_to_num(-y * np.log(o) - (1-y) * np.log(1-o)))\n",
    "\n",
    "\n",
    "def squared_error(o, y):\n",
    "    \"\"\" Squared error loss function\n",
    "    :param o: The output\n",
    "    :param y: The expected output\n",
    "    :return: The square error - the loss\n",
    "    \"\"\"\n",
    "    return 0.5 * ((y-o) ** 2).sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYx0U9NZnH0y"
   },
   "source": [
    "## Test 1 - Learning XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JM6HvXgnNmq"
   },
   "source": [
    " [0, 0] => 0\n",
    " \n",
    " [0, 1] => 1\n",
    " \n",
    " [1, 0] => 1\n",
    " \n",
    " [1, 1] => 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1524871110380,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "OM5WjjgMfRJV",
    "outputId": "320bb1ba-f10a-459a-c1bf-fa3d904b8959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | error : 0.112\n",
      "epoch 200 | error : 0.095\n",
      "epoch 300 | error : 0.078\n",
      "epoch 400 | error : 0.035\n",
      "epoch 500 | error : 0.012\n",
      "epoch 600 | error : 0.006\n",
      "epoch 700 | error : 0.003\n",
      "epoch 800 | error : 0.002\n",
      "epoch 900 | error : 0.002\n",
      "epoch 1000 | error : 0.001\n",
      "epoch 1100 | error : 0.001\n",
      "epoch 1200 | error : 0.001\n",
      "epoch 1300 | error : 0.001\n",
      "epoch 1400 | error : 0.001\n",
      "epoch 1500 | error : 0.001\n",
      "epoch 1600 | error : 0.001\n",
      "epoch 1700 | error : 0.000\n",
      "epoch 1800 | error : 0.000\n",
      "epoch 1900 | error : 0.000\n",
      "========================================\n",
      "[0 0] | expected : 0 | output : 0.005\n",
      "[0 1] | expected : 1 | output : 0.962\n",
      "[1 0] | expected : 1 | output : 0.961\n",
      "[1 1] | expected : 0 | output : 0.003\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A test for Multi Layer Perceptron\n",
    "\n",
    "An assignment of Connectionist Computing - UCD COMP30230 Module\n",
    "\n",
    "- Test 1 to see if mlp learns the XOR problem\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.1'\n",
    "__author__ = 'Qirun Chen - Student No. 16212138'\n",
    "__date__ = '27 Apr 2018'\n",
    "\n",
    "\n",
    "# Initialize the XOR inputs\n",
    "XOR_inputs = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "# Split the inputs and outputs\n",
    "x = XOR_inputs[:, :-1]\n",
    "y = XOR_inputs[:, -1]\n",
    "\n",
    "# Initialize the MLP network\n",
    "mlp = MLP(2, 2, 1, max_epochs=2000, verbose=3)\n",
    "\n",
    "# Training the MLP\n",
    "mlp.fit(x, y)\n",
    "\n",
    "# Predict the MLP on XOR inputs\n",
    "prediction = mlp.predict(x)\n",
    "print(\"==\" * 20)\n",
    "for i, l in enumerate(y):\n",
    "    print('%s | expected : %.f | output : %.3f' % (str(x[i]), y[i], prediction[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgDoxFD75Bej"
   },
   "source": [
    "## Test 2 - Learning Sin function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llmNbQsg5JQb"
   },
   "source": [
    "### sin(x1-x2+x3-x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49449,
     "status": "ok",
     "timestamp": 1524871420449,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "d8KxNfLdfSpM",
    "outputId": "d8f3ea42-0b96-46be-9905-9765832058ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | error : 0.006\n",
      "epoch 200 | error : 0.001\n",
      "epoch 300 | error : 0.000\n",
      "epoch 400 | error : 0.000\n",
      "epoch 500 | error : 0.000\n",
      "epoch 600 | error : 0.000\n",
      "epoch 700 | error : 0.000\n",
      "epoch 800 | error : 0.000\n",
      "epoch 900 | error : 0.000\n",
      "epoch 1000 | error : 0.000\n",
      "epoch 1100 | error : 0.000\n",
      "epoch 1200 | error : 0.000\n",
      "epoch 1300 | error : 0.000\n",
      "epoch 1400 | error : 0.000\n",
      "epoch 1500 | error : 0.000\n",
      "epoch 1600 | error : 0.000\n",
      "epoch 1700 | error : 0.000\n",
      "epoch 1800 | error : 0.000\n",
      "epoch 1900 | error : 0.000\n",
      "epoch 2000 | error : 0.000\n",
      "epoch 2100 | error : 0.000\n",
      "epoch 2200 | error : 0.000\n",
      "epoch 2300 | error : 0.000\n",
      "epoch 2400 | error : 0.000\n",
      "epoch 2500 | error : 0.000\n",
      "epoch 2600 | error : 0.000\n",
      "epoch 2700 | error : 0.000\n",
      "epoch 2800 | error : 0.000\n",
      "epoch 2900 | error : 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7f4d90f35630>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A test for Multi Layer Perceptron\n",
    "\n",
    "An assignment of Connectionist Computing - UCD COMP30230 Module\n",
    "\n",
    "- Test 2 to see if mlp learns the Sin function\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.1'\n",
    "__author__ = 'Qirun Chen - Student No. 16212138'\n",
    "__date__ = '27 Apr 2018'\n",
    "\n",
    "\n",
    "# Initialize the MLP network\n",
    "mlp = MLP(4, 10, 1, learning_rate=0.1, max_epochs=3000, verbose=3)\n",
    "\n",
    "# Initialize the training set and test set\n",
    "sample_num = 500\n",
    "split_ratio = 0.8\n",
    "train_num = int(sample_num * split_ratio)\n",
    "test_num = sample_num - train_num\n",
    "\n",
    "# Randomly initialize the inputs\n",
    "X = np.random.uniform(-1, 1, (sample_num, 4))\n",
    "# Compute the output according to the function sin(x1-x2+x3-x4)\n",
    "y = list(map(lambda a: np.sin(a[0] - a[1] + a[2] - a[3]), X))\n",
    "# Stack the output to the x values\n",
    "ds = np.column_stack((X, y))\n",
    "\n",
    "# Split the training set and test set\n",
    "train = ds[:train_num]\n",
    "test = ds[train_num:]\n",
    "\n",
    "# Train the MLP network\n",
    "mlp.fit(train[:, :-1], train[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1524871663104,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "YwnCFJCgN4ZQ",
    "outputId": "c19774ae-d7fe-4585-ae2b-a2dced6ce6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21052972  0.02015795  0.76271994 -0.45459273] | expected : 1 | output : 0.968\n",
      "[-0.53144349 -0.03693457 -0.32118328 -0.98627899] | expected : 0 | output : 0.183\n",
      "[-0.15788841  0.99115722 -0.66410057  0.22267419] | expected : -1 | output : -0.910\n",
      "[ 0.90034523  0.49676132  0.39518066 -0.28629993] | expected : 1 | output : 0.894\n",
      "[-0.8190109  -0.66919025 -0.2892418  -0.91594012] | expected : 0 | output : 0.444\n",
      "[ 0.21667442 -0.09182284 -0.08227725 -0.43948194] | expected : 1 | output : 0.605\n",
      "[ 0.59136831  0.36976902 -0.06531821 -0.50908552] | expected : 1 | output : 0.604\n",
      "[-0.71851288  0.17740553  0.9249166  -0.83790149] | expected : 1 | output : 0.766\n",
      "[ 0.51128773  0.5391894   0.35820463 -0.24778348] | expected : 1 | output : 0.537\n",
      "[ 0.12768396 -0.50392547 -0.754824    0.83719861] | expected : -1 | output : -0.827\n",
      "[-0.73209477  0.01689286  0.96435228 -0.84309403] | expected : 1 | output : 0.887\n",
      "[-0.96506678 -0.81170764 -0.19286978 -0.41397669] | expected : 0 | output : 0.073\n",
      "[ 0.74254604  0.93239048  0.37241519 -0.00300495] | expected : 0 | output : 0.199\n",
      "[-0.08040226 -0.13374836 -0.13538274  0.05536698] | expected : -0 | output : -0.141\n",
      "[-0.06717616 -0.89807615 -0.19053825 -0.26239971] | expected : 1 | output : 0.795\n",
      "[-0.69060025  0.11516379 -0.28176663 -0.40199231] | expected : -1 | output : -0.629\n",
      "[-0.11027823  0.12535076 -0.89350339  0.32402137] | expected : -1 | output : -0.970\n",
      "[-0.24515098 -0.18218414 -0.10183365  0.70011758] | expected : -1 | output : -0.765\n",
      "[-0.08175081  0.79175262 -0.21028942 -0.95701437] | expected : -0 | output : -0.113\n",
      "[-0.42144139  0.49604047  0.83491934 -0.84060963] | expected : 1 | output : 0.683\n",
      "[-0.90732429 -0.87251216 -0.63921656 -0.55719491] | expected : -0 | output : -0.112\n",
      "[ 0.69214814  0.07522348  0.40016066 -0.13178445] | expected : 1 | output : 0.921\n",
      "[ 0.66427943  0.44209189 -0.54098096 -0.85194352] | expected : 1 | output : 0.506\n",
      "[ 0.57591977 -0.03789632 -0.41890893 -0.55575771] | expected : 1 | output : 0.674\n",
      "[ 0.5314297  -0.03846399  0.31488878  0.32067603] | expected : 1 | output : 0.520\n",
      "[ 0.07129981  0.92334946 -0.90163966  0.78641524] | expected : -1 | output : -0.559\n",
      "[-0.54585302 -0.6256471   0.9417845  -0.25800616] | expected : 1 | output : 0.953\n",
      "[-0.88666807 -0.19423095 -0.17415052 -0.05994614] | expected : -1 | output : -0.725\n",
      "[ 0.88899879  0.85068206 -0.32697962  0.6585185 ] | expected : -1 | output : -0.819\n",
      "[-0.116497    0.03444056 -0.09381296 -0.37449778] | expected : 0 | output : 0.141\n",
      "[ 0.27452681 -0.21750886  0.71650043  0.18653314] | expected : 1 | output : 0.869\n",
      "[-0.68636412 -0.79169484 -0.50570055 -0.89797274] | expected : 0 | output : 0.463\n",
      "[ 0.54456539  0.20723052 -0.79173873  0.75671736] | expected : -1 | output : -0.939\n",
      "[-0.90317995 -0.83242729 -0.55723732 -0.25929079] | expected : -0 | output : -0.357\n",
      "[ 0.60496938  0.73041405 -0.62574474 -0.90562694] | expected : 0 | output : 0.179\n",
      "[0.98704368 0.98120033 0.34218908 0.53898864] | expected : -0 | output : -0.200\n",
      "[-0.69387343 -0.23422858  0.80313024  0.23619669] | expected : 0 | output : 0.114\n",
      "[ 4.74647364e-01 -7.90870501e-01  2.31379971e-04 -7.21748353e-01] | expected : 1 | output : 0.918\n",
      "[-0.02622384  0.42174947 -0.21188185 -0.23800031] | expected : -0 | output : -0.403\n",
      "[ 0.10961818  0.90835471 -0.2585437   0.4387386 ] | expected : -1 | output : -0.972\n",
      "[ 0.13228     0.50329919 -0.77745529 -0.6535397 ] | expected : -0 | output : -0.456\n",
      "[ 0.31214178 -0.194826    0.61701726  0.48094352] | expected : 1 | output : 0.583\n",
      "[-0.979793   -0.10095455  0.0162883  -0.36136835] | expected : -0 | output : -0.480\n",
      "[ 0.49183647  0.31460802  0.28739765 -0.52818125] | expected : 1 | output : 0.845\n",
      "[ 0.57456275 -0.55115413 -0.09270801 -0.07751616] | expected : 1 | output : 0.909\n",
      "[-0.77913191 -0.94043816 -0.33962437 -0.32802708] | expected : 0 | output : 0.157\n",
      "[-0.37611826 -0.72111485  0.37588069  0.40692447] | expected : 0 | output : 0.308\n",
      "[ 0.49471599 -0.66760238 -0.40901053 -0.62668823] | expected : 1 | output : 0.964\n",
      "[-0.99614792 -0.06517342 -0.46993253 -0.94285013] | expected : -0 | output : -0.436\n",
      "[-0.06182758  0.84133602  0.85866941  0.72996984] | expected : -1 | output : -0.704\n",
      "[-0.29565377  0.95199785  0.10154104 -0.58148841] | expected : -1 | output : -0.525\n",
      "[0.21033507 0.57645811 0.62111615 0.16081016] | expected : 0 | output : 0.104\n",
      "[ 0.60871302 -0.36665001 -0.60753769 -0.79388105] | expected : 1 | output : 0.924\n",
      "[ 0.70492036 -0.56654829  0.99954587  0.59465228] | expected : 1 | output : 0.972\n",
      "[ 0.75274985 -0.43644779 -0.6211053   0.09160516] | expected : 0 | output : 0.456\n",
      "[ 0.47734456 -0.30193302  0.53689091 -0.11600957] | expected : 1 | output : 0.969\n",
      "[0.03104926 0.63663501 0.37817123 0.96395044] | expected : -1 | output : -0.933\n",
      "[-0.39393302  0.49630114  0.63110727  0.85716667] | expected : -1 | output : -0.908\n",
      "[ 0.00511763 -0.41294503 -0.76586449 -0.08247921] | expected : -0 | output : -0.261\n",
      "[ 0.14373974 -0.31590024  0.97131897  0.20640131] | expected : 1 | output : 0.946\n",
      "[ 0.34809725  0.49408263  0.11360798 -0.310741  ] | expected : 0 | output : 0.287\n",
      "[-0.28465112  0.81171824 -0.59110507 -0.46457294] | expected : -1 | output : -0.945\n",
      "[ 0.14786078 -0.32810969 -0.40795425  0.41652353] | expected : -0 | output : -0.346\n",
      "[-0.09838706 -0.05288882  0.10892001 -0.98423601] | expected : 1 | output : 0.875\n",
      "[-0.44073492  0.53643717 -0.54905568 -0.72276456] | expected : -1 | output : -0.723\n",
      "[ 0.47035674 -0.21893124 -0.97425616  0.15313811] | expected : -0 | output : -0.418\n",
      "[ 0.92266244  0.86678589 -0.14713399  0.10026825] | expected : -0 | output : -0.190\n",
      "[-0.88959624  0.25561163  0.14885709  0.7079132 ] | expected : -1 | output : -0.964\n",
      "[ 0.81516132  0.71702574 -0.72802554 -0.80392556] | expected : 0 | output : 0.197\n",
      "[ 0.74910828 -0.53685493 -0.10283408  0.89656884] | expected : 0 | output : 0.276\n",
      "[-0.56505363  0.48683464  0.60307573  0.95251431] | expected : -1 | output : -0.965\n",
      "[-0.02083534 -0.8493676   0.61996485  0.53354207] | expected : 1 | output : 0.806\n",
      "[-0.62596462 -0.17429271 -0.86485275 -0.69127751] | expected : -1 | output : -0.571\n",
      "[-0.98975016 -0.03509212  0.39479962 -0.46565551] | expected : -0 | output : -0.093\n",
      "[-0.64807406  0.75938479 -0.23476875  0.99184143] | expected : -0 | output : -0.489\n",
      "[ 0.98488089  0.43648436 -0.73904401 -0.63690833] | expected : 0 | output : 0.438\n",
      "[-0.7254147   0.50467412 -0.05875446  0.96185995] | expected : -1 | output : -0.807\n",
      "[ 0.5383401  -0.62733836 -0.29331057  0.7164985 ] | expected : 0 | output : 0.152\n",
      "[ 0.96490941 -0.67322025  0.16024816 -0.60986348] | expected : 1 | output : 0.711\n",
      "[-0.91306798 -0.11629939 -0.6142163  -0.48646829] | expected : -1 | output : -0.811\n",
      "[-0.79307971 -0.5258442   0.11122665  0.63810907] | expected : -1 | output : -0.714\n",
      "[ 0.36887472  0.97355303  0.43697638 -0.09044007] | expected : -0 | output : -0.070\n",
      "[ 0.24242529  0.29041401 -0.44274767 -0.45635193] | expected : -0 | output : -0.024\n",
      "[ 0.4155061  -0.5173256   0.44822138 -0.80031119] | expected : 1 | output : 0.840\n",
      "[ 0.60134482  0.88961268 -0.80289979 -0.23787133] | expected : -1 | output : -0.754\n",
      "[-0.59378352  0.18293435 -0.90997642  0.44795415] | expected : -1 | output : -0.871\n",
      "[ 0.08232514  0.29057559 -0.91699775  0.19102283] | expected : -1 | output : -0.960\n",
      "[-0.74853573  0.40126964  0.69851554  0.25533486] | expected : -1 | output : -0.648\n",
      "[0.51301738 0.70858967 0.25341452 0.77135358] | expected : -1 | output : -0.649\n",
      "[-0.43234734  0.75827752 -0.45034085 -0.121382  ] | expected : -1 | output : -0.972\n",
      "[-0.93296888  0.72436654  0.47431044 -0.32940946] | expected : -1 | output : -0.763\n",
      "[-0.48952514 -0.38144069 -0.33832069  0.1873476 ] | expected : -1 | output : -0.585\n",
      "[-0.81254437 -0.94883691 -0.6038545   0.59331698] | expected : -1 | output : -0.880\n",
      "[-0.79265968 -0.93823869  0.40693575 -0.21680244] | expected : 1 | output : 0.689\n",
      "[-0.18849367 -0.82811092  0.39457581  0.68308468] | expected : 0 | output : 0.340\n",
      "[ 0.14301924  0.77575892 -0.84692717  0.55275246] | expected : -1 | output : -0.916\n",
      "[ 0.9734934  -0.04928302  0.57653553  0.6078194 ] | expected : 1 | output : 0.846\n",
      "[ 0.52474427 -0.20764514  0.80677437  0.92938673] | expected : 1 | output : 0.553\n",
      "[ 0.97741273  0.19450192 -0.55279529  0.9647272 ] | expected : -1 | output : -0.661\n",
      "[ 0.39058864 -0.52577608  0.59783406  0.01208894] | expected : 1 | output : 0.972\n",
      "========================================\n",
      "error on test set : 0.00009\n"
     ]
    }
   ],
   "source": [
    "test_x = test[:, :-1]\n",
    "test_y = test[:, -1]\n",
    "# Predict on the test set\n",
    "prediction = mlp.predict(test_x).flatten()\n",
    "\n",
    "cost = 0.\n",
    "for i, l in enumerate(test_x):\n",
    "    cost += 0.5 * (test_y[i] - prediction[i]) ** 2\n",
    "    print('%s | expected : %.f | output : %.3f' % \n",
    "          (str(l), test_y[i], prediction[i]))\n",
    "print('==' * 20)\n",
    "print('error on test set : %.5f' % (cost/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1524871581777,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "lU51gDO-fVG-",
    "outputId": "d14a0a88-6934-4c25-ed41-1a1eccb1e9d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAD4CAYAAAAn8XUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d909c0128>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the x value for the sin function for plotting\n",
    "x = list(map(lambda a: a[0] - a[1] + a[2] - a[3], X))\n",
    "output_y = mlp.predict(X)\n",
    "# Indicate the figure size\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Draw the real function and the MLP prediction approximated function\n",
    "plt.plot(x, y, 'g^', x, output_y, 'ro')\n",
    "# X axis [0, 3] | Y axis [0, 1.3]\n",
    "plt.axis([0, 3, 0, 1.3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hT32rpJ_6UJO"
   },
   "source": [
    "## Exceptional Test - Letter Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3456517,
     "status": "ok",
     "timestamp": 1524906394542,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "zyCA_xBn6fxO",
    "outputId": "937adbe7-ded8-4c16-97d5-38cbc93bf7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | error : 0.720 | accuracy : 0.862\n",
      "epoch 200 | error : 0.680 | accuracy : 0.868\n",
      "epoch 300 | error : 0.684 | accuracy : 0.869\n",
      "epoch 400 | error : 0.707 | accuracy : 0.870\n",
      "epoch 500 | error : 0.715 | accuracy : 0.862\n",
      "epoch 600 | error : 0.733 | accuracy : 0.855\n",
      "epoch 700 | error : 0.730 | accuracy : 0.849\n",
      "epoch 800 | error : 0.743 | accuracy : 0.841\n",
      "epoch 900 | error : 0.755 | accuracy : 0.846\n",
      "epoch 1000 | error : 0.742 | accuracy : 0.848\n",
      "epoch 1100 | error : 0.785 | accuracy : 0.851\n",
      "epoch 1200 | error : 0.784 | accuracy : 0.842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7f0c47b979e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import csv\n",
    "\n",
    "def to_character(n):\n",
    "    \"\"\" Map an integer to the character according to ASCII table\n",
    "    :param n: The int number in ASCII\n",
    "    :return: The character in ASCII\n",
    "    \"\"\"\n",
    "    # The starting number of character 'A' in ASCII table\n",
    "    char_delta = ord('A')\n",
    "    return chr(int(n) + char_delta)\n",
    "\n",
    "\n",
    "dataset = list()\n",
    "# Read data from csv\n",
    "with open('./letter-recognition.csv', newline='') as data_file:\n",
    "    reader = csv.reader(data_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        # Map the character to the integer number\n",
    "        row[0] = ord(row[0]) - ord('A')\n",
    "        dataset.append(row)\n",
    "\n",
    "# Define a ratio to split the training test and test set\n",
    "training_split = int(len(dataset) * 0.8)\n",
    "# Split the training set\n",
    "training_set = np.array(dataset[:training_split], dtype=np.int)\n",
    "X = training_set[:, 1:]\n",
    "y = training_set[:, 0]\n",
    "\n",
    "# Define the number of output labels\n",
    "num_labels = 26\n",
    "\n",
    "# One hot encode y labels\n",
    "categorical_y = np.zeros((len(y), num_labels))\n",
    "for i, l in enumerate(y):\n",
    "    categorical_y[i][l] = 1\n",
    "\n",
    "# Normalize the features - Important!!!\n",
    "X = X/15\n",
    "\n",
    "# find a proper configuration\n",
    "mlp = MLP(16, 30, 26, learning_rate=0.1, max_epochs=1300, verbose=3)\n",
    "mlp.fit(X, categorical_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1524909573024,
     "user": {
      "displayName": "Qirun Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107057765158730970246"
     },
     "user_tz": -60
    },
    "id": "-OZgjtKs6ofu",
    "outputId": "9e8261c8-ada8-48c2-d60d-57d98e7ab21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: U | Output: M\n",
      "Expected: M | Output: M\n",
      "Expected: I | Output: I\n",
      "Expected: S | Output: S\n",
      "Expected: N | Output: N\n",
      "Expected: N | Output: N\n",
      "Expected: J | Output: J\n",
      "Expected: R | Output: R\n",
      "Expected: L | Output: L\n",
      "Expected: M | Output: M\n",
      "Expected: O | Output: O\n",
      "Expected: H | Output: M\n",
      "Expected: P | Output: F\n",
      "Expected: J | Output: X\n",
      "========================================\n",
      "Test sample size: 4000 | Correctly predicted sample size: 3210\n",
      "Accuracy: 0.802\n",
      "========================================\n",
      "A => Sample Number: 156 | Correct Number: 132 | Accuracy: 0.846\n",
      "B => Sample Number: 136 | Correct Number: 114 | Accuracy: 0.838\n",
      "C => Sample Number: 142 | Correct Number: 108 | Accuracy: 0.761\n",
      "D => Sample Number: 167 | Correct Number: 146 | Accuracy: 0.874\n",
      "E => Sample Number: 152 | Correct Number: 115 | Accuracy: 0.757\n",
      "F => Sample Number: 153 | Correct Number: 119 | Accuracy: 0.778\n",
      "G => Sample Number: 164 | Correct Number: 127 | Accuracy: 0.774\n",
      "H => Sample Number: 151 | Correct Number: 75 | Accuracy: 0.497\n",
      "I => Sample Number: 165 | Correct Number: 142 | Accuracy: 0.861\n",
      "J => Sample Number: 148 | Correct Number: 121 | Accuracy: 0.818\n",
      "K => Sample Number: 146 | Correct Number: 112 | Accuracy: 0.767\n",
      "L => Sample Number: 157 | Correct Number: 128 | Accuracy: 0.815\n",
      "M => Sample Number: 144 | Correct Number: 130 | Accuracy: 0.903\n",
      "N => Sample Number: 166 | Correct Number: 127 | Accuracy: 0.765\n",
      "O => Sample Number: 139 | Correct Number: 113 | Accuracy: 0.813\n",
      "P => Sample Number: 168 | Correct Number: 130 | Accuracy: 0.774\n",
      "Q => Sample Number: 168 | Correct Number: 150 | Accuracy: 0.893\n",
      "R => Sample Number: 161 | Correct Number: 124 | Accuracy: 0.770\n",
      "S => Sample Number: 161 | Correct Number: 116 | Accuracy: 0.720\n",
      "T => Sample Number: 151 | Correct Number: 120 | Accuracy: 0.795\n",
      "U => Sample Number: 168 | Correct Number: 136 | Accuracy: 0.810\n",
      "V => Sample Number: 136 | Correct Number: 120 | Accuracy: 0.882\n",
      "W => Sample Number: 139 | Correct Number: 121 | Accuracy: 0.871\n",
      "X => Sample Number: 159 | Correct Number: 126 | Accuracy: 0.792\n",
      "Y => Sample Number: 145 | Correct Number: 126 | Accuracy: 0.869\n",
      "Z => Sample Number: 158 | Correct Number: 132 | Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Split the inputs and outputs of the test set\n",
    "test_set = np.array(dataset[training_split:], dtype=np.int)\n",
    "test_x = test_set[:, 1:]\n",
    "# Normalization - Important!!!\n",
    "test_x = test_x / 15\n",
    "test_y = test_set[:, 0]\n",
    "# Predict on the normalized test set\n",
    "prediction = mlp.predict(test_x)\n",
    "\n",
    "# Print prediction details\n",
    "# Initialize the confusion dictionary\n",
    "confusion_dict = {to_character(i): 0 for i in range(26)}\n",
    "letter_num_dict = {to_character(i): 0 for i in range(26)}\n",
    "for i, _ in enumerate(test_y):\n",
    "\n",
    "    letter_num_dict[to_character(test_y[i])] += 1\n",
    "    # Print some predictions\n",
    "    if i % 300 == 0:\n",
    "        print('Expected: %s | Output: %s' % (to_character(test_y[i]), to_character(prediction[i])))\n",
    "    if test_y[i] == prediction[i]:\n",
    "        confusion_dict[to_character(prediction[i])] += 1\n",
    "        \n",
    "\n",
    "print('==' * 20)\n",
    "# Calculate the accuracy\n",
    "accuracy = sum(confusion_dict.values()) / len(test_y)\n",
    "print('Test sample size: %d | Correctly predicted sample size: %d' % \n",
    "      (len(test_y), sum(confusion_dict.values())))\n",
    "print('Accuracy: %.3f' % accuracy)\n",
    "\n",
    "# Performance on each class\n",
    "print('==' * 20)\n",
    "for k,v in letter_num_dict.items():\n",
    "    print('%s => Sample Number: %d | Correct Number: %d | Accuracy: %.3f' %\n",
    "         (k, v, confusion_dict[k], confusion_dict[k]/v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pvZfoIK5g6F3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "letter_reco.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
